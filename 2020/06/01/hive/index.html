<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
      
    
    
  <script src="true"></script>
  <link href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="0.前置知识121）SQL技能：如MySQL2）Hadoop框架：HDFS + MapReduce  1. Hive是什么1Hive是FaceBook开源的海量结构化数据的分析框架，Hive的本质是将结构化的数据映射为一张表，最终表将翻译成MR程序，底层还是通过MR作为计算引擎，HDFS作为存储，YARN作为资源调度  2. Hive的架构1Hive计算的数据存储在HDFS中，Hive的元数据信息">
<meta property="og:type" content="article">
<meta property="og:title" content="hive">
<meta property="og:url" content="http://yoursite.com/2020/06/01/hive/index.html">
<meta property="og:site_name" content="柒7七">
<meta property="og:description" content="0.前置知识121）SQL技能：如MySQL2）Hadoop框架：HDFS + MapReduce  1. Hive是什么1Hive是FaceBook开源的海量结构化数据的分析框架，Hive的本质是将结构化的数据映射为一张表，最终表将翻译成MR程序，底层还是通过MR作为计算引擎，HDFS作为存储，YARN作为资源调度  2. Hive的架构1Hive计算的数据存储在HDFS中，Hive的元数据信息">
<meta property="article:published_time" content="2020-06-01T12:37:52.000Z">
<meta property="article:modified_time" content="2020-06-08T12:48:31.232Z">
<meta property="article:author" content="柒7七">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/01/hive/"/>





  <title>hive | 柒7七</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	<a href="https://github.com/zhanghanting" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">柒7七</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/01/hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="柒7七">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2021/01/06/UpLEnikCWguPrz1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="柒7七">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">hive</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-01T20:37:52+08:00">
                2020-06-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop生态</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/06/01/hive/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/06/01/hive/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10,202
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  46
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="0-前置知识"><a href="#0-前置知识" class="headerlink" title="0.前置知识"></a>0.前置知识</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1）SQL技能：如MySQL</span><br><span class="line">2）Hadoop框架：HDFS + MapReduce</span><br></pre></td></tr></table></figure>

<h3 id="1-Hive是什么"><a href="#1-Hive是什么" class="headerlink" title="1. Hive是什么"></a>1. Hive是什么</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive是FaceBook开源的海量结构化数据的分析框架，Hive的本质是将结构化的数据映射为一张表，最终表将翻译成MR程序，底层还是通过MR作为计算引擎，HDFS作为存储，YARN作为资源调度</span><br></pre></td></tr></table></figure>

<h3 id="2-Hive的架构"><a href="#2-Hive的架构" class="headerlink" title="2. Hive的架构"></a>2. Hive的架构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive计算的数据存储在HDFS中，Hive的元数据信息（表的信息）存储在第三方的数据库中，默认使用的是Derby，可以切换成其它数据库，如MySQL</span><br></pre></td></tr></table></figure>

<h4 id="2-1-架构原理"><a href="#2-1-架构原理" class="headerlink" title="2.1 架构原理"></a>2.1 架构原理</h4><p>​    Hive通过给用户提供一系列交互接口，接受到用户的指令（SQL），使用自己的Driver，结合元数据（MetaStore），将这些指令翻译成MapReduce，提交到Hadoop中执行，最后将执行返回的结果输出到用户交互接口。</p>
<h3 id="3-Hive数据类型"><a href="#3-Hive数据类型" class="headerlink" title="3. Hive数据类型"></a>3. Hive数据类型</h3><h4 id="3-1基本数据类型"><a href="#3-1基本数据类型" class="headerlink" title="3.1基本数据类型"></a>3.1基本数据类型</h4><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SAMLLINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型true or false</td>
<td>TRUE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.12</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.1524</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMSTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>​         对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。  </p>
<h4 id="3-2集合数据类型"><a href="#3-2集合数据类型" class="headerlink" title="3.2集合数据类型"></a>3.2集合数据类型</h4><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和C语言中的struct类似，都可以通过点符号访问元素内容。例如，如果某个列的数据类型是STRUTCT{first STRING,last STRING}，那么第1个元素可以通过字段.first来引用，struct的item就是指里面的属性</td>
<td><code>struct&lt;street:string,city:string&gt;</code></td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段[‘last’]获取最后一个元素</td>
<td><code>map&lt;string,int&gt;</code></td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,’Doe’]，那么第2个元素可以通过数组名[1]进行引用</td>
<td><code>array&lt;string&gt;</code></td>
</tr>
</tbody></table>
<h3 id="4-DDL数据定义"><a href="#4-DDL数据定义" class="headerlink" title="4. DDL数据定义"></a>4. DDL数据定义</h3><h3 id="5-DML数据操作"><a href="#5-DML数据操作" class="headerlink" title="5. DML数据操作"></a>5. DML数据操作</h3><h3 id="6-查询"><a href="#6-查询" class="headerlink" title="6. 查询"></a>6. 查询</h3><h4 id="6-1-全局排序"><a href="#6-1-全局排序" class="headerlink" title="6.1 全局排序"></a>6.1 全局排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">order by：全局排序，只有一个reducer</span><br><span class="line">ASC：升序（默认）</span><br><span class="line">DESC：降序</span><br></pre></td></tr></table></figure>

<h4 id="6-2-每个MapReduce内部排序-Sort-by"><a href="#6-2-每个MapReduce内部排序-Sort-by" class="headerlink" title="6.2 每个MapReduce内部排序(Sort by)"></a>6.2 每个MapReduce内部排序(Sort by)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sort by:对于大规模的数据集order by的效率非常低。在很多情况下。并不需要全局排序，此时可以使用sort by</span><br><span class="line">sort by为每个reducer产生一个排序文件。每个reducer内部进行排序</span><br><span class="line"></span><br><span class="line"> 关注点: 有多个reducer，也就是有多个分区</span><br><span class="line"> 注意点: 有多个reducer,单独使用sort by, 数据会被随机分到每个reducer中，在每个reducer中sort by会将数据排序。</span><br></pre></td></tr></table></figure>

<p>示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）设置reduce个数</span><br><span class="line">hive (default)&gt; set mapreduce.job.reduces&#x3D;3;</span><br><span class="line">2）根据部门编号降序查看员工信息</span><br><span class="line">hive (default)&gt; select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure>

<h4 id="6-3-分区排序（Distribute-by）"><a href="#6-3-分区排序（Distribute-by）" class="headerlink" title="6.3 分区排序（Distribute by）"></a>6.3 分区排序（Distribute by）</h4><p>​    Distribute By：在有些情况下，我们需要控制某个特定行应该要到哪个reducer，通常是为了进行后续的聚集操作。distribute by字句可以做这件事。distribute by类似MR中的partition（自定义分区），进行分区，结合sort by使用。</p>
<p>​    对于distribute by进行测试，一定要分配多reduce处理，否则无法看到distribute by的效果</p>
<p>示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from emp distribute by deptno  sort by  empno desc ;</span><br></pre></td></tr></table></figure>

<h4 id="6-4-cluster-by-分区排序"><a href="#6-4-cluster-by-分区排序" class="headerlink" title="6.4 cluster by 分区排序"></a>6.4 cluster by 分区排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">关注点: 相当于distribute by 和sort by同时用， 并且分区和排序的字段是同一个，并且排序是升序的情况.</span><br><span class="line"></span><br><span class="line">select * from emp distribute by deptno sort by deptno asc ; </span><br><span class="line">等同于</span><br><span class="line">select * from emp cluster by deptno ;</span><br></pre></td></tr></table></figure>



<h3 id="7-分区表和分桶表"><a href="#7-分区表和分桶表" class="headerlink" title="7. 分区表和分桶表"></a>7. 分区表和分桶表</h3><h4 id="7-1-分区表"><a href="#7-1-分区表" class="headerlink" title="7.1 分区表"></a>7.1 分区表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分区表实际就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive分区表的分区其实就是分目录，把一个大的数据集根据业务需求分割成小的数据集。在查询时通过where子句中的表达式选择查询所需要的指定分区，就不用暴力扫描所有的数据了，这样查询效率会提高很多。</span><br></pre></td></tr></table></figure>

<h4 id="7-1-1-分区表基本操作"><a href="#7-1-1-分区表基本操作" class="headerlink" title="7.1.1 分区表基本操作"></a>7.1.1 分区表基本操作</h4><p>1）引入分区表的数据（需求是根据日志对日志进行管理，通过部门信息模拟）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">dept_20200402.log</span><br><span class="line">dept_20200403.log</span><br></pre></td></tr></table></figure>

<p>2）创建分区表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;create table dept_partition(deptno int,dname string,loc string) </span><br><span class="line">partitioned by(day string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>3）加载数据到分区表中</p>
<p>（1）数据准备</p>
<p>dept_20200401.log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br></pre></td></tr></table></figure>

<p>dept_20200402.log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br></pre></td></tr></table></figure>

<p>dept_20200403.log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">50	TEST	2000</span><br><span class="line">60	DEV	1900</span><br></pre></td></tr></table></figure>



<p>（2）加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200401.log&#39; into table dept_partition partition(day&#x3D;&#39;20200401&#39;);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200402.log&#39; into table dept_partition partition(day&#x3D;&#39;20200402&#39;);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200403.log&#39; into table dept_partition partition(day&#x3D;&#39;20200403&#39;);</span><br></pre></td></tr></table></figure>



<p>4）</p>
<p>单分区查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day&#x3D;&#39;20200401&#39;;</span><br></pre></td></tr></table></figure>



<p>多分区联合查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day&#x3D;&#39;20200401&#39;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day&#x3D;&#39;20200402&#39;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day&#x3D;&#39;20200403&#39;;</span><br><span class="line">              </span><br><span class="line">hive (default)&gt; select * from dept_partition where day&#x3D;&#39;20200401&#39; or</span><br><span class="line">                day&#x3D;&#39;20200402&#39; or day&#x3D;&#39;20200403&#39; ;</span><br></pre></td></tr></table></figure>



<p>5）增加分区</p>
<p>创建单个分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; alter table dept_partition add partition(day&#x3D;&#39;20200404&#39;);</span><br></pre></td></tr></table></figure>

<p>同时创建多个分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; alter table dept_partition add partition(day&#x3D;&#39;20200405&#39;) partition(day&#x3D;&#39;20200406&#39;);</span><br></pre></td></tr></table></figure>

<p>6）删除分区</p>
<p>删除单个分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; alter table dept_partition drop partition(day&#x3D;&#39;20200404&#39;);</span><br></pre></td></tr></table></figure>

<p>删除多个分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;alter table dept_partition drop partition(day&#x3D;&#39;20200405&#39;),parition(day&#x3D;&#39;20200406&#39;);</span><br></pre></td></tr></table></figure>

<p>7）查看分区表有多少分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;show partitions dept_partition;</span><br></pre></td></tr></table></figure>

<p>8）查看分区表结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br></pre></td></tr></table></figure>

<h4 id="7-1-2-分区表注意事项"><a href="#7-1-2-分区表注意事项" class="headerlink" title="7.1.2 分区表注意事项"></a>7.1.2 分区表注意事项</h4><p>如果一天的日志数据量很大，也是可以创建二级分区表的</p>
<p>1）创建二级分区表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; create table dept_parition2(deptno int,dname string,loc string)</span><br><span class="line">partitioned by (day string,hour string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>2）正常的加载数据</p>
<p>（1）加载数据到二级分区表中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200401.log&#39; into table dept_parition2 partition(day&#x3D;&#39;20200401&#39;,hour&#x3D;&#39;12&#39;);</span><br></pre></td></tr></table></figure>

<p>（2）查询分区数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select * from dept_partition2 where day&#x3D;&#39;20200401&#39; and hour &#x3D; &#39;12&#39;;</span><br></pre></td></tr></table></figure>

<p>3）把数据上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<p>（1）方式一：上传数据后修复</p>
<p>上传数据到HDFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mydb.db&#x2F;dept_partition2&#x2F;day&#x3D;20200401&#x2F;hour&#x3D;13;</span><br><span class="line"> </span><br><span class="line">hive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200401.log  </span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mydb.db&#x2F;dept_partition2&#x2F;day&#x3D;20200401&#x2F;hour&#x3D;13;</span><br></pre></td></tr></table></figure>

<p>查询数据（查询不到刚上传的数据，因为没有相应的元数据信息）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where day&#x3D;&#39;20200401&#39; and hour&#x3D;&#39;13&#39;;</span><br></pre></td></tr></table></figure>

<p>执行修复命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure>

<p>再次查询数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where day&#x3D;&#39;20200401&#39; and hour&#x3D;&#39;13&#39;;</span><br></pre></td></tr></table></figure>



<p>（2）方式二：上传数据后添加分区</p>
<p>上传数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mydb.db&#x2F;dept_partition2&#x2F;day&#x3D;20200401&#x2F;hour&#x3D;14;</span><br><span class="line"> </span><br><span class="line">hive (default)&gt; dfs -put &#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200401.log  &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mydb.db&#x2F;dept_partition2&#x2F;day&#x3D;20200401&#x2F;hour&#x3D;14;</span><br></pre></td></tr></table></figure>

<p>执行添加分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(day&#x3D;&#39;201709&#39;,hour&#x3D;&#39;14&#39;);</span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where day&#x3D;&#39;20200401&#39; and hour&#x3D;&#39;14&#39;;</span><br></pre></td></tr></table></figure>

<h4 id="7-1-3-动态分区调整"><a href="#7-1-3-动态分区调整" class="headerlink" title="7.1.3 动态分区调整"></a>7.1.3 动态分区调整</h4><p>​    关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p>
<p>1）开启动态分区参数设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">（1）开启动态分区功能（默认true，开启）</span><br><span class="line">hive.exec.dynamic.partition&#x3D;true</span><br><span class="line">（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</span><br><span class="line">hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br><span class="line">（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</span><br><span class="line">hive.exec.max.dynamic.partitions&#x3D;1000</span><br><span class="line">（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</span><br><span class="line">hive.exec.max.dynamic.partitions.pernode&#x3D;100</span><br><span class="line">（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000</span><br><span class="line">hive.exec.max.created.files&#x3D;100000</span><br><span class="line">（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false</span><br><span class="line">hive.error.on.empty.partition&#x3D;false</span><br></pre></td></tr></table></figure>

<p>2）案例实操</p>
<p>​    需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p>
<p>（1）创建目标分区表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; create table dept_partition_dy(id int,name string)</span><br><span class="line">partitioned by(loc int)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>（2）设置动态分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode &#x3D; nonstrict;</span><br><span class="line">hive(default)&gt; insert into table dept_partition_dy partition(loc) </span><br><span class="line">select deptno,dname,loc from dept;</span><br></pre></td></tr></table></figure>



<h3 id="7-2-分桶表"><a href="#7-2-分桶表" class="headerlink" title="7.2 分桶表"></a>7.2 分桶表</h3><p>​    分区表提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可以形成合力的分区。对于一张表或分区，Hive可进一步组织成桶，也就是更细粒度的数据范围划分。</p>
<p>​    分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p>​    分区针对的是数据的存储路径，分为多个目录，而分桶针对是数据文件，会把原来的文件根据分桶规则将数据分到不同的数据文件。</p>
<p>1）先创建分桶表，通过直接导入数据文件的方式</p>
<p>（1）数据准备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure>

<p>（2）创建分桶表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table stu_buck(id int,name string)</span><br><span class="line">clustered by(id)</span><br><span class="line">into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>（3）查看表结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default) &gt; desc formatted stu_buck;</span><br></pre></td></tr></table></figure>

<p>（4）导入数据到分桶表中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;dive&#x2F;datas&#x2F;student.txt&#39; into table stu_buck;</span><br></pre></td></tr></table></figure>

<p>（5）通过web到对应的目录下查看创建的分通表中是否分为4个桶</p>
<p>（6）查询分桶的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select * from stu_buck;</span><br></pre></td></tr></table></figure>

<p>分桶规则：HIve的分桶采用对分桶字段的值进行哈希，然后除以桶的个数取余的方式决定该条记录存放在哪个桶当中</p>
<h3 id="7-3-抽样查询"><a href="#7-3-抽样查询" class="headerlink" title="7.3 抽样查询"></a>7.3 抽样查询</h3><p>​    对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。可以通过对表进行抽样来满足这个需求。</p>
<p>​    查询表stu_buck中的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<p>注：tablesample是抽样语句，语法：tablesample(bucket x out of y)</p>
<p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽象的比例。例如：table总共分了4份，当y=2时，抽取（4/2）=2个bucket 的数据，当y=8时，抽取（4/8）=1/个bucket的数据。</p>
<p>x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4。</p>
<p>x的值必须小于y的值</p>
<h3 id="8-函数"><a href="#8-函数" class="headerlink" title="8. 函数"></a>8. 函数</h3><h4 id="8-1-系统内置函数"><a href="#8-1-系统内置函数" class="headerlink" title="8.1 系统内置函数"></a>8.1 系统内置函数</h4><p>1）查看系统自带的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; show functions;</span><br></pre></td></tr></table></figure>

<p>2）显示自带函数的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; desc function upper;</span><br></pre></td></tr></table></figure>

<p>3）详细显示自带的函数的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; desc function extended upper;</span><br></pre></td></tr></table></figure>



<h4 id="8-2-常用内置函数"><a href="#8-2-常用内置函数" class="headerlink" title="8.2 常用内置函数"></a>8.2 常用内置函数</h4><h5 id="8-2-1-空字段复制"><a href="#8-2-1-空字段复制" class="headerlink" title="8.2.1 空字段复制"></a>8.2.1 空字段复制</h5><h5 id="8-2-2-CASE-WHEN"><a href="#8-2-2-CASE-WHEN" class="headerlink" title="8.2.2 CASE WHEN"></a>8.2.2 CASE WHEN</h5><h5 id="8-2-3-行转列"><a href="#8-2-3-行转列" class="headerlink" title="8.2.3 行转列"></a>8.2.3 行转列</h5><p>1）相关函数说明</p>
<p>​    CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串。</p>
<p>​    CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间。</p>
<p>​    COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="8-2-4-列转行"><a href="#8-2-4-列转行" class="headerlink" title="8.2.4 列转行"></a>8.2.4 列转行</h5><p>1）函数说明</p>
<p>​    split()：将给定的字符串，通过给定的分隔符进行分割，返回array</p>
<p>​    explode()：将一列中的array或者map等拆分成多行</p>
<p>​    lateral view：侧写</p>
<p>2）数据准备</p>
<p>表6-7 数据准备</p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p>3）需求</p>
<p>​    将电影分类中的数组数据展开，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure>

<p>4）创建本地movie.txt，导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">atguigu@hadoop102 datas]$ vi movie_info.txt</span><br><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》	悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》	战争,动作,灾难</span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table movie_info(</span><br><span class="line">    movie string, </span><br><span class="line">    category string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;movie_info.txt&quot; into table movie_info;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select movie,category_name</span><br><span class="line">from movie_info</span><br><span class="line">lateral view</span><br><span class="line">explode(split(category,&quot;,&quot;))movie_info_tpm as category_name;</span><br></pre></td></tr></table></figure>



<h5 id="8-2-5-窗口函数"><a href="#8-2-5-窗口函数" class="headerlink" title="8.2.5 窗口函数"></a>8.2.5 窗口函数</h5><p>1）相关函数说明</p>
<p>​    over():指定分析函数工作的数据窗口大小，这个数据窗口的大小可能会随着行的变化而变化</p>
<p>​    current row：当前行</p>
<p>​    n preceding：往前n行数据</p>
<p>​    n following：往后n行数据</p>
<p>​    unbounded：起点</p>
<p>​        unbounded preceding 表示从前面的起点</p>
<p>​        unbounded following 表示到后面的终点</p>
<p>​    lag(col,n,default_val)：往前第n行数据，没有数据则用默认值代替</p>
<p>​    lead(col,default_val)：往后第n行数据，没有数据则用默认值代替</p>
<p>​    ntile(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，ntile返回此行所属的组的编号。其中n必须为Int类型</p>
<p>2）数据准备：name，orderdate，cost</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure>

<p>3）创建本地business.txt，导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vim business.txt</span><br></pre></td></tr></table></figure>

<p>4）创建hive表并导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table business(</span><br><span class="line">name string, </span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;</span><br><span class="line">create table business(</span><br><span class="line">name string, </span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;</span><br><span class="line">load data local inpath &quot;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;business.txt&quot; into table business;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据<br>（1）查询在2017年4月份购买过的顾客及总人数,输出如下<br>     NAME    total<br>     jack     2<br>     tony     2</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--分析：</span></span><br><span class="line"><span class="comment">--a.首先将2017年4月份的数据查出来</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span> <span class="keyword">from</span> business <span class="keyword">where</span> orderdate <span class="keyword">like</span> <span class="string">'2017-04%'</span>;</span><br><span class="line"><span class="comment">--或者</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span> <span class="keyword">from</span> business <span class="keyword">where</span> <span class="keyword">MONTH</span>(orderdate) = <span class="string">'04'</span>;</span><br><span class="line"><span class="comment">--b.然后将4月份的数据总人数通过开窗函数查出来(这里是对字表t1的数据开窗)</span></span><br><span class="line"><span class="keyword">select</span> t1.name,<span class="keyword">count</span>(t1.name) <span class="keyword">over</span>()</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span> <span class="keyword">from</span> business <span class="keyword">where</span> <span class="keyword">MONTH</span>(orderdate) = <span class="string">'04'</span>)t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t1.name;</span><br></pre></td></tr></table></figure>

<p>（2）查询顾客的购买明细及月购买总额（这里是指所有顾客的月购买总额）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) allcustomer_month_sum_cost</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（3）查询顾客的购买明细及月购买总额（这里指每个顾客的月购买总额）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>,<span class="keyword">month</span>(orderdate)) current_customer_month_costs</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（4）每个顾客的购买明细，及所有顾客的cost按照日期进行累加</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--分析：为每条数据的开窗的大小为：从结果集开始的位置到当前行</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate)sum_cost</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"><span class="comment">--等同于</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>)sum_cost</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（5）每个顾客的购买明细    及  </p>
<p>​        所有顾客的cost按照日期累加</p>
<p>​        所有顾客的cost按照日期上一行与当前行的累加</p>
<p>​        所有顾客的cost按照日期当前行与下一行的累加</p>
<p>​        所有顾客的cost按照日期上一行 当前行 下一行的累加</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) sum_costs1,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) sum_cost2,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) sum_cost3,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) sum_cost4</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（6）每个顾客的购买明细 及</p>
<p>​        所有顾客的cost按照日期    上一行与下一行的累加</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--分析</span></span><br><span class="line"><span class="comment">--a.将每条数据的上一行和下一行的cost提取到当前行</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">lag(<span class="keyword">cost</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) prev_cost,</span><br><span class="line"><span class="keyword">lead</span>(<span class="keyword">cost</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) next_cost</span><br><span class="line"><span class="keyword">from</span> business;            <span class="comment">-----&gt; t1</span></span><br><span class="line"><span class="comment">--b.求prev_cost和next_cost的和</span></span><br><span class="line"><span class="keyword">select</span> t1.name,t1.orderdate,t1.cost,t1.prev_cost,t1.next_cost,t1.prev_cost+t1.next_cost p_n_cost</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">lag(<span class="keyword">cost</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) prev_cost,</span><br><span class="line"><span class="keyword">lead</span>(<span class="keyword">cost</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) next_cost</span><br><span class="line"><span class="keyword">from</span> business)t1;</span><br></pre></td></tr></table></figure>

<p>（7）查询每个顾客上次的购买时间和下次的购买时间</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">cost</span>,orderdate current_od,</span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) prev_od,</span><br><span class="line"><span class="keyword">lead</span>(orderdate,<span class="number">1</span>,<span class="string">'9999-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) next_od</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（8）查询每个顾客上次的购买时间和下次的购买时间 及每个顾客的cost按照日期累加</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">cost</span>,orderdate current_od,</span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) prev_od,</span><br><span class="line"><span class="keyword">lead</span>(orderdate,<span class="number">1</span>,<span class="string">'9999-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) next_od,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) sum_cost</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<p>（9）查询前20%时间的订单信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--按订单时间排序，分为5组然后取第一组</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) gid</span><br><span class="line"><span class="keyword">from</span> business;          <span class="comment">----&gt;t1</span></span><br><span class="line"><span class="comment">--取5组里面的第一组数据，即前20%的数据</span></span><br><span class="line"><span class="keyword">select</span> t1.name,t1.orderdate,t1.cost</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) gid</span><br><span class="line"><span class="keyword">from</span> business)t1</span><br><span class="line"><span class="keyword">where</span> t1.gid = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>



<h5 id="8-2-6-Rank"><a href="#8-2-6-Rank" class="headerlink" title="8.2.6 Rank"></a>8.2.6 Rank</h5><p>1）相关函数说明</p>
<p>​    rank()：考虑并列情况，总数不会变</p>
<p>​    dense_rank()：考虑并列情况，总数会较少</p>
<p>​    row_number()：按照顺序排序</p>
<p>2）数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>subject</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>语文</td>
<td>87</td>
</tr>
<tr>
<td>孙悟空</td>
<td>数学</td>
<td>95</td>
</tr>
<tr>
<td>孙悟空</td>
<td>英语</td>
<td>68</td>
</tr>
<tr>
<td>大海</td>
<td>语文</td>
<td>94</td>
</tr>
<tr>
<td>大海</td>
<td>数学</td>
<td>56</td>
</tr>
<tr>
<td>大海</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>宋宋</td>
<td>语文</td>
<td>64</td>
</tr>
<tr>
<td>宋宋</td>
<td>数学</td>
<td>86</td>
</tr>
<tr>
<td>宋宋</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>婷婷</td>
<td>语文</td>
<td>65</td>
</tr>
<tr>
<td>婷婷</td>
<td>数学</td>
<td>85</td>
</tr>
<tr>
<td>婷婷</td>
<td>英语</td>
<td>78</td>
</tr>
</tbody></table>
<p>4）创建本地score.txt,导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vim score.txt</span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score int) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;score.txt&#39; into table score;</span><br></pre></td></tr></table></figure>

<p>6）需求：计算每科成绩排名</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rk,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drk,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rn</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>



<h5 id="8-2-7-其它常用函数"><a href="#8-2-7-其它常用函数" class="headerlink" title="8.2.7 其它常用函数"></a>8.2.7 其它常用函数</h5><h4 id="8-3-自定义函数"><a href="#8-3-自定义函数" class="headerlink" title="8.3 自定义函数"></a>8.3 自定义函数</h4><p>1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p>
<p>2）此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</p>
<p>3）根据用户自定义函数类别分为以下三种</p>
<p>（1）UDF（User-Defined Function）</p>
<p>​    一进一出</p>
<p>（2）UDAF（User-Defined Aggregation Function）</p>
<p>​    聚集函数：多进一出 类似于 count/max/min</p>
<p>（3）UDTF（User-Defined Table_Generating Functions）</p>
<p>​    一进多出  如lateral view explode()</p>
<p>4）官方文档地址</p>
<p>5）编程步骤</p>
<p>（1）继承Hive提供的类</p>
<p>​    org.apache.hadoop.hive.ql.udf.generic.GenericUDF</p>
<p>​    org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p>
<p>（2）实现类中的抽象方法</p>
<p>（3）在hive的命令行窗口创建函数</p>
<p>添加jar</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure>

<p>创建function和刚添加的jar关联起来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function name as class_name;</span><br></pre></td></tr></table></figure>

<p>（4）在hive的命令行窗口删除函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop [temporary] function [if exists] [dbname.]function name;</span><br></pre></td></tr></table></figure>





<h4 id="8-4-自定义UDF函数"><a href="#8-4-自定义UDF函数" class="headerlink" title="8.4 自定义UDF函数"></a>8.4 自定义UDF函数</h4><p>0）需求：</p>
<p>​    自定义一个UDF实现计算给定字符串的长度，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select my_len(&quot;abcd&quot;);</span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<p>1）创建一个maven工程hive</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）创建一个类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*	自定义UDF函数，需要继承GenericUDF类</span></span><br><span class="line"><span class="comment">*	需求：计算指定字符串的长度</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyStringLength</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arguments 输入参数类型的鉴别器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回值类型的鉴别器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span><span class="keyword">throws</span> UDFArgumentException</span>&#123;</span><br><span class="line">        <span class="comment">//判断输入参数的个数</span></span><br><span class="line">        <span class="keyword">if</span>(arguments.length != <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLegthException(<span class="string">"Input Args Length Error"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//判断输入参数的类型</span></span><br><span class="line">        <span class="keyword">if</span>(!argument[<span class="number">0</span>].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;</span><br><span class="line">             <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">0</span>,<span class="string">"Input Args Type Error!!!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//函数本身返回值为int,需要返回int类型鉴别器对象</span></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arguments 输入的参数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回值</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferedObject[] arguments)</span><span class="keyword">throws</span> HiveException</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(arguments[<span class="number">0</span>].get() == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> argument[<span class="number">0</span>].get.toString().length();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）打成jar包上传到服务器/opt/module/hive/datas/myudf.jar</p>
<p>5）将jar包添加到hive的classpath</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; add jar &#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar</span><br></pre></td></tr></table></figure>

<p>6）创建临时函数与开发好的java class关联</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; create temporary function my_len as &quot;com.atguigu.hive.MyStringLength&quot;;</span><br></pre></td></tr></table></figure>

<p>7）即可在hql使用自定义的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select my_len(&#39;abcd&#39;);</span><br><span class="line">4</span><br></pre></td></tr></table></figure>



<h4 id="8-5-自定义UDTF函数"><a href="#8-5-自定义UDTF函数" class="headerlink" title="8.5 自定义UDTF函数"></a>8.5 自定义UDTF函数</h4><p>0）需求</p>
<p>自定义一个UDTF实现将一个任意分隔符的字符串切割成独立的单词，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;, &quot;,&quot;);</span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br></pre></td></tr></table></figure>

<p>1）代码实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive.udtf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义UDTF函数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 需要继承 GenericUDTF类，并重写抽象方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 函数的使用: select myudtf('hive,hadoop,flume,kafka',',');</span></span><br><span class="line"><span class="comment"> *      结果:  word</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *            hive</span></span><br><span class="line"><span class="comment"> *            hadoop</span></span><br><span class="line"><span class="comment"> *            flume</span></span><br><span class="line"><span class="comment"> *            kafka</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  扩展:</span></span><br><span class="line"><span class="comment"> *  select myudtf2('hadoop-niupi,java-lihai,songsong-kuai,dahai-lang',',','-');</span></span><br><span class="line"><span class="comment"> *  结果: word1    word2</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *       hadoop   niupi</span></span><br><span class="line"><span class="comment"> *       java     lihai</span></span><br><span class="line"><span class="comment"> *       songsong kuai</span></span><br><span class="line"><span class="comment"> *       dahai    lang</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringSplitUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">     <span class="keyword">private</span> ArrayList&lt;String&gt; outList  = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法  约定函数的返回值类型  和 函数的返回值列名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> argOIs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span><span class="keyword">throws</span> UDFArgumentException</span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//基本判断</span></span><br><span class="line">        List&lt;? extends StructField&gt; fieldRefs = argOIs.getAllStructFieldRefs();</span><br><span class="line">		<span class="keyword">if</span>(fieldRefs.size()!=<span class="number">2</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentException(<span class="string">"Input Args Length Error!!！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//约定函数返回的列的名字</span></span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    	fieldNames.add(<span class="string">"word"</span>);</span><br><span class="line">        <span class="comment">//约定函数返回的列的类型</span></span><br><span class="line">         ArrayList&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">         <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取参数</span></span><br><span class="line">        String argsData =  args[<span class="number">0</span>].toString();</span><br><span class="line">        String argsSplit = args[<span class="number">1</span>].toString();</span><br><span class="line">        <span class="comment">//切分数据</span></span><br><span class="line">        String[] words = argsData.split(argsSplit);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出数据 每个单词一行</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">//因为集合是重用的，所以每次要先清空</span></span><br><span class="line">            outList.clear();</span><br><span class="line">            <span class="comment">//将当前的单词放到集合中</span></span><br><span class="line">            outList.add(word);</span><br><span class="line">            <span class="comment">//将当前的单词作为一行写出去</span></span><br><span class="line">            forward(outList);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需求扩展</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select myudtf2(&#39;hadoop-niupi,java-lihai,songsong-kuai,dahai-lang&#39;,&#39;,&#39;,&#39;-&#39;);</span><br><span class="line"> 结果: word1    word2</span><br><span class="line">      hadoop   niupi</span><br><span class="line">      java     lihai</span><br><span class="line">      songsong kuai</span><br><span class="line">      dahai    lang</span><br></pre></td></tr></table></figure>

<p>代码实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringSplitUDTF2</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;String&gt; outList  = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法  约定函数的返回值类型  和 函数的返回值列名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> argOIs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="comment">//基本判断</span></span><br><span class="line">        List&lt;? extends StructField&gt; fieldRefs = argOIs.getAllStructFieldRefs();</span><br><span class="line">        <span class="keyword">if</span>(fieldRefs.size()!=<span class="number">3</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentException(<span class="string">"Input Args Length Error!!!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//约定函数返回的列的名字</span></span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldNames.add(<span class="string">"word1"</span>);</span><br><span class="line">        fieldNames.add(<span class="string">"word2"</span>);</span><br><span class="line">        <span class="comment">//约定函数返回的列的类型</span></span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        <span class="comment">//获取参数</span></span><br><span class="line">        String argsData = args[<span class="number">0</span>].toString(); <span class="comment">// hadoop-niupi,java-lihai,songsong-kuai,dahai-lang</span></span><br><span class="line">        String rowSplit = args[<span class="number">1</span>].toString(); <span class="comment">// ,</span></span><br><span class="line">        String colSplit = args[<span class="number">2</span>].toString(); <span class="comment">// -</span></span><br><span class="line">        <span class="comment">//切分数据</span></span><br><span class="line">        String[] rows = argsData.split(rowSplit);</span><br><span class="line">        <span class="keyword">for</span> (String row : rows) &#123;</span><br><span class="line">            <span class="comment">//因为集合是复用的，使用前先清空</span></span><br><span class="line">            outList.clear();</span><br><span class="line">            <span class="comment">// row : hadoop-niupi</span></span><br><span class="line">            String[] cols = row.split(colSplit);</span><br><span class="line">            <span class="keyword">for</span> (String word : cols) &#123;</span><br><span class="line">                outList.add(word);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//写出</span></span><br><span class="line">            forward(outList);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="9-压缩和存储"><a href="#9-压缩和存储" class="headerlink" title="9. 压缩和存储"></a>9. 压缩和存储</h3><h4 id="9-1-Hadoop压缩配置"><a href="#9-1-Hadoop压缩配置" class="headerlink" title="9.1 Hadoop压缩配置"></a>9.1 Hadoop压缩配置</h4><h5 id="9-1-1-MR支持的压缩编码"><a href="#9-1-1-MR支持的压缩编码" class="headerlink" title="9.1.1 MR支持的压缩编码"></a>9.1.1 MR支持的压缩编码</h5><table>
<thead>
<tr>
<th>压缩格式</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
<p>用到比较多的是snappy和lzo</p>
<h5 id="9-1-2-压缩参数配置"><a href="#9-1-2-压缩参数配置" class="headerlink" title="9.1.2 压缩参数配置"></a>9.1.2 压缩参数配置</h5><p>​    要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs    （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec,  org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h4 id="9-2-开启Map端输出阶段压缩（MR引擎）"><a href="#9-2-开启Map端输出阶段压缩（MR引擎）" class="headerlink" title="9.2 开启Map端输出阶段压缩（MR引擎）"></a>9.2 开启Map端输出阶段压缩（MR引擎）</h4><p>​    开启map输出阶段压缩可以减少job中map和ReduceTask间的数据传输率。具体配置如下</p>
<p>1）案例实操</p>
<p>（1）开启hive中间传输数据压缩功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>（2）开启MapReduce中map输出压缩功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>（3）设置MapReduce中map输出数据的压缩方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec&#x3D;</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）执行查询语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure>



<h4 id="9-3-开启Reduce输出阶段压缩"><a href="#9-3-开启Reduce输出阶段压缩" class="headerlink" title="9.3 开启Reduce输出阶段压缩"></a>9.3 开启Reduce输出阶段压缩</h4><p>​    当Hive将输出写到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<p>1）案例实操</p>
<p>（1）开启hive最终输出数据压缩功能  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>（2）开启mapreduce最终输出数据压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>（3）设置mapreduce最终数据输出压缩方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec &#x3D;</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）设置mapreduce最终数据输出压缩为块压缩  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type&#x3D;BLOCK;</span><br></pre></td></tr></table></figure>

<p>（5）测试一下输出结果是否是压缩文件  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;distribute-result&#39; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<h4 id="9-4-文件存储格式"><a href="#9-4-文件存储格式" class="headerlink" title="9.4 文件存储格式"></a>9.4 文件存储格式</h4><p>​    HIve支持的存储数据的格式有：TEXTFILE,SEQUENCEFILE,QRC,PARQUET。</p>
<h4 id="9-4-1-列式存储和行式存储"><a href="#9-4-1-列式存储和行式存储" class="headerlink" title="9.4.1 列式存储和行式存储"></a>9.4.1 列式存储和行式存储</h4><p>1）行存储的特点</p>
<p>​    查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p>2）列存储的特点</p>
<p>​    因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p>​    TEXTFIlE和SEQUENCEFILE的存储格式都是基于行存储的</p>
<p>​    ORC和PARQUET是基于列式存储的</p>
<p>​    因为通常查询的时候大多数情况都不会去查询所有的字段，而是查询某些属性，或对某几个字段进行计算，所以大多数情况还是列式存储，这样查询效率会更快一点。</p>
<h5 id="9-4-2-TextFile格式"><a href="#9-4-2-TextFile格式" class="headerlink" title="9.4.2 TextFile格式"></a>9.4.2 TextFile格式</h5><p>​    默认格式，数据不做压缩，磁盘开销大，磁盘开销大，数据解析开销大。可结合Gzip和Bzip2使用，但是用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h4 id="9-4-3-Orc格式"><a href="#9-4-3-Orc格式" class="headerlink" title="9.4.3 Orc格式"></a>9.4.3 Orc格式</h4><p>​    Orc(Optimized Row Columnar)是Hive0.11版里引入的新的存储格式。</p>
<p>​    每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer。</p>
<p>1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p>
<p>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>​    每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h5 id="9-4-4-Parquet格式"><a href="#9-4-4-Parquet格式" class="headerlink" title="9.4.4 Parquet格式"></a>9.4.4 Parquet格式</h5><p>​    Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式是自解析的。</p>
<p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念  </p>
<p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p>
<p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。  </p>
<p>​    通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。</p>
<h5 id="9-4-5-主流文件存储格式对比试验"><a href="#9-4-5-主流文件存储格式对比试验" class="headerlink" title="9.4.5 主流文件存储格式对比试验"></a>9.4.5 主流文件存储格式对比试验</h5><p>1）准备测试数据</p>
<p>2）TextFile</p>
<p>（1）创建表，存储数据格式为TEXTFILE</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;log.data&#39; into table log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_text;</span><br></pre></td></tr></table></figure>

<p>3）ORC</p>
<p>（11）创建表，存储数据格式为ORC</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(<span class="string">"orc.compress"</span> = <span class="string">"NONE"</span>);<span class="comment">--设置ORC存储不能使用压缩</span></span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;</span><br></pre></td></tr></table></figure>

<p>4）Parquet</p>
<p>（1）创建表，存储数据格式为parquet</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_parquet(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as parquet;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_parquet&#x2F; ;</span><br></pre></td></tr></table></figure>

<p>存储文件的对比总结：</p>
<p>ORC &gt; Parquet &gt; textFile</p>
<p>可以通过测试发现这三种格式存储的文件查询速度相近</p>
<h4 id="9-5-存储和压缩结合"><a href="#9-5-存储和压缩结合" class="headerlink" title="9.5 存储和压缩结合"></a>9.5 存储和压缩结合</h4><h4 id="9-5-1-测试存储和压缩"><a href="#9-5-1-测试存储和压缩" class="headerlink" title="9.5.1 测试存储和压缩"></a>9.5.1 测试存储和压缩</h4><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level  compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes  in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>268,435,456</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create  row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be  created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<p>​    注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</p>
<p>1）创建一个ZLIB压缩的ORC存储方式</p>
<p>（1）建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc_zlib(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;&#x3D;&quot;ZLIB&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into log_orc_zlib select * from log_text;</span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_zlib&#x2F; ;</span><br></pre></td></tr></table></figure>

<p>2）创建一个snappy压缩的ORC存储方式</p>
<p>（1）建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;&#x3D;&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into log_orc_snappy select * from log_text;</span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_snappy&#x2F; ;</span><br></pre></td></tr></table></figure>

<p>……</p>
<p>4）存储方式和压缩总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</span><br></pre></td></tr></table></figure>



<h3 id="10-企业级调优"><a href="#10-企业级调优" class="headerlink" title="10 . 企业级调优"></a>10 . 企业级调优</h3><h3 id="11-Hive实战"><a href="#11-Hive实战" class="headerlink" title="11. Hive实战"></a>11. Hive实战</h3><h4 id="11-1-需求描述"><a href="#11-1-需求描述" class="headerlink" title="11.1 需求描述"></a>11.1 需求描述</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<p>– 统计视频观看数Top10</p>
<p>– 统计视频类别热度Top10</p>
<p>– 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</p>
<p>– 统计视频观看数Top50所关联视频的所属类别Rank</p>
<p>– 统计每个类别中的视频热度Top10,以Music为例</p>
<p>– 统计每个类别视频观看数Top10</p>
<p>– 统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频 </p>
<h4 id="11-2-数据结构"><a href="#11-2-数据结构" class="headerlink" title="11.2 数据结构"></a>11.2 数据结构</h4><p>1）视频表</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id（String）</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者（String）</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄（int）</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别（Array<String>）</td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度（Int）</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数（Int）</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分（Double）</td>
<td>满分5分</td>
</tr>
<tr>
<td>Ratings</td>
<td>流量（Int）</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数（Int）</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedId</td>
<td>相关视频id（Array<String>）</td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
<p>2）用户表</p>
<table>
<thead>
<tr>
<th><strong>字段</strong></th>
<th><strong>备注</strong></th>
<th><strong>字段类型</strong></th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量  （关注的人）</td>
<td>int</td>
</tr>
</tbody></table>
<h4 id="11-3-准备工作"><a href="#11-3-准备工作" class="headerlink" title="11.3 准备工作"></a>11.3 准备工作</h4><h5 id="11-3-1-ETL"><a href="#11-3-1-ETL" class="headerlink" title="11.3.1 ETL"></a>11.3.1 ETL</h5><p>​    通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&amp;”分割，同时去掉两边空格，多个相关视频id也使用“&amp;”进行分割。</p>
<p>1）ETL之封装工具类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> 	* 数据清洗方法</span></span><br><span class="line"><span class="comment"> 	*/</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span>  String  <span class="title">etlData</span><span class="params">(String srcData)</span></span>&#123;</span><br><span class="line">        StringBuffer resultData = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">        <span class="comment">//1. 先将数据通过\t 切割</span></span><br><span class="line">        String[] datas = srcData.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">//2. 判断长度是否小于9</span></span><br><span class="line">        <span class="keyword">if</span>(datas.length &lt;<span class="number">9</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//3. 将数据中的视频类别的空格去掉</span></span><br><span class="line">        datas[<span class="number">3</span>]=datas[<span class="number">3</span>].replaceAll(<span class="string">" "</span>,<span class="string">""</span>);</span><br><span class="line">        <span class="comment">//4. 将数据中的关联视频id通过&amp;拼接</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(i &lt; <span class="number">9</span>)&#123;</span><br><span class="line">                <span class="comment">//4.1 没有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">"\t"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//4.2 有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">"&amp;"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> resultData.toString();</span><br><span class="line">    &#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>2）ETL之Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 清洗谷粒影音的原始数据</span></span><br><span class="line"><span class="comment"> * 清洗规则</span></span><br><span class="line"><span class="comment"> *  1. 将数据长度小于9的清洗掉</span></span><br><span class="line"><span class="comment"> *  2. 将数据中的视频类别中间的空格去掉   People &amp; Blogs</span></span><br><span class="line"><span class="comment"> *  3. 将数据中的关联视频id通过&amp;符号拼接</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">//清洗</span></span><br><span class="line">        String resultData = ETLUtil.etlData(line);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(resultData != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//写出</span></span><br><span class="line">            k.set(resultData);</span><br><span class="line">            context.write(k,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）ETL之Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">package</span> com.atguigu.gulivideo.etl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job  = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(EtlDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(EtlMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）将ETL程序打包为etl.jar并上传到Linux的/opt/module/hive/datas目录下</p>
<p>5）上传原始数据到HDFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas] pwd</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas</span><br><span class="line">[atguigu@hadoop102 datas] hadoop fs -mkdir -p  &#x2F;gulivideo&#x2F;video</span><br><span class="line">[atguigu@hadoop102 datas] hadoop fs -mkdir -p  &#x2F;gulivideo&#x2F;user</span><br><span class="line">[atguigu@hadoop102 datas] hadoop fs -put gulivideo&#x2F;user&#x2F;user.txt   &#x2F;gulivideo&#x2F;user</span><br><span class="line">[atguigu@hadoop102 datas] hadoop fs -put gulivideo&#x2F;video&#x2F;*.txt   &#x2F;gulivideo&#x2F;video</span><br></pre></td></tr></table></figure>

<p>6）ETL数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas] hadoop jar  etl.jar  com.atguigu.hive.etl.EtlDriver &#x2F;gulivideo&#x2F;video &#x2F;gulivideo&#x2F;video&#x2F;output</span><br></pre></td></tr></table></figure>

<h5 id="11-3-2-准备表"><a href="#11-3-2-准备表" class="headerlink" title="11.3.2 准备表"></a>11.3.2 准备表</h5><p>​    1）需要准备的表</p>
<p>​    2）创建原始数据表</p>
<p>​    3）创建ORC存储格式带snappy压缩的表</p>
<h5 id="11-3-3-安装Tez引擎"><a href="#11-3-3-安装Tez引擎" class="headerlink" title="11.3.3 安装Tez引擎"></a>11.3.3 安装Tez引擎</h5>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate comment here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="https://i.loli.net/2021/01/22/2p8ZleOfkJqT75w.png" alt="柒7七 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/05/30/Hadoop-HA/" rel="next" title="Hadoop HA">
                <i class="fa fa-chevron-left"></i> Hadoop HA
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/06/Hive%E5%AE%9E%E6%88%98/" rel="prev" title="Hive实战">
                Hive实战 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.loli.net/2021/01/06/UpLEnikCWguPrz1.jpg"
                alt="柒7七" />
            
              <p class="site-author-name" itemprop="name">柒7七</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">106</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhanghanting" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/" target="_blank" title="weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>weibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://baidu.com/" title="百度" target="_blank">百度</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#0-前置知识"><span class="nav-number">1.</span> <span class="nav-text">0.前置知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Hive是什么"><span class="nav-number">2.</span> <span class="nav-text">1. Hive是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Hive的架构"><span class="nav-number">3.</span> <span class="nav-text">2. Hive的架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-架构原理"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 架构原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Hive数据类型"><span class="nav-number">4.</span> <span class="nav-text">3. Hive数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1基本数据类型"><span class="nav-number">4.1.</span> <span class="nav-text">3.1基本数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2集合数据类型"><span class="nav-number">4.2.</span> <span class="nav-text">3.2集合数据类型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-DDL数据定义"><span class="nav-number">5.</span> <span class="nav-text">4. DDL数据定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-DML数据操作"><span class="nav-number">6.</span> <span class="nav-text">5. DML数据操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-查询"><span class="nav-number">7.</span> <span class="nav-text">6. 查询</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-全局排序"><span class="nav-number">7.1.</span> <span class="nav-text">6.1 全局排序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-每个MapReduce内部排序-Sort-by"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 每个MapReduce内部排序(Sort by)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-分区排序（Distribute-by）"><span class="nav-number">7.3.</span> <span class="nav-text">6.3 分区排序（Distribute by）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-cluster-by-分区排序"><span class="nav-number">7.4.</span> <span class="nav-text">6.4 cluster by 分区排序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-分区表和分桶表"><span class="nav-number">8.</span> <span class="nav-text">7. 分区表和分桶表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-分区表"><span class="nav-number">8.1.</span> <span class="nav-text">7.1 分区表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-1-分区表基本操作"><span class="nav-number">8.2.</span> <span class="nav-text">7.1.1 分区表基本操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-2-分区表注意事项"><span class="nav-number">8.3.</span> <span class="nav-text">7.1.2 分区表注意事项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-3-动态分区调整"><span class="nav-number">8.4.</span> <span class="nav-text">7.1.3 动态分区调整</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-分桶表"><span class="nav-number">9.</span> <span class="nav-text">7.2 分桶表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-抽样查询"><span class="nav-number">10.</span> <span class="nav-text">7.3 抽样查询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-函数"><span class="nav-number">11.</span> <span class="nav-text">8. 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-系统内置函数"><span class="nav-number">11.1.</span> <span class="nav-text">8.1 系统内置函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-常用内置函数"><span class="nav-number">11.2.</span> <span class="nav-text">8.2 常用内置函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-1-空字段复制"><span class="nav-number">11.2.1.</span> <span class="nav-text">8.2.1 空字段复制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-2-CASE-WHEN"><span class="nav-number">11.2.2.</span> <span class="nav-text">8.2.2 CASE WHEN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-3-行转列"><span class="nav-number">11.2.3.</span> <span class="nav-text">8.2.3 行转列</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-4-列转行"><span class="nav-number">11.2.4.</span> <span class="nav-text">8.2.4 列转行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-5-窗口函数"><span class="nav-number">11.2.5.</span> <span class="nav-text">8.2.5 窗口函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-6-Rank"><span class="nav-number">11.2.6.</span> <span class="nav-text">8.2.6 Rank</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2-7-其它常用函数"><span class="nav-number">11.2.7.</span> <span class="nav-text">8.2.7 其它常用函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-自定义函数"><span class="nav-number">11.3.</span> <span class="nav-text">8.3 自定义函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-自定义UDF函数"><span class="nav-number">11.4.</span> <span class="nav-text">8.4 自定义UDF函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-自定义UDTF函数"><span class="nav-number">11.5.</span> <span class="nav-text">8.5 自定义UDTF函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-压缩和存储"><span class="nav-number">12.</span> <span class="nav-text">9. 压缩和存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-Hadoop压缩配置"><span class="nav-number">12.1.</span> <span class="nav-text">9.1 Hadoop压缩配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#9-1-1-MR支持的压缩编码"><span class="nav-number">12.1.1.</span> <span class="nav-text">9.1.1 MR支持的压缩编码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#9-1-2-压缩参数配置"><span class="nav-number">12.1.2.</span> <span class="nav-text">9.1.2 压缩参数配置</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-开启Map端输出阶段压缩（MR引擎）"><span class="nav-number">12.2.</span> <span class="nav-text">9.2 开启Map端输出阶段压缩（MR引擎）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-3-开启Reduce输出阶段压缩"><span class="nav-number">12.3.</span> <span class="nav-text">9.3 开启Reduce输出阶段压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-文件存储格式"><span class="nav-number">12.4.</span> <span class="nav-text">9.4 文件存储格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-1-列式存储和行式存储"><span class="nav-number">12.5.</span> <span class="nav-text">9.4.1 列式存储和行式存储</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#9-4-2-TextFile格式"><span class="nav-number">12.5.1.</span> <span class="nav-text">9.4.2 TextFile格式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-3-Orc格式"><span class="nav-number">12.6.</span> <span class="nav-text">9.4.3 Orc格式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#9-4-4-Parquet格式"><span class="nav-number">12.6.1.</span> <span class="nav-text">9.4.4 Parquet格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#9-4-5-主流文件存储格式对比试验"><span class="nav-number">12.6.2.</span> <span class="nav-text">9.4.5 主流文件存储格式对比试验</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-5-存储和压缩结合"><span class="nav-number">12.7.</span> <span class="nav-text">9.5 存储和压缩结合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-5-1-测试存储和压缩"><span class="nav-number">12.8.</span> <span class="nav-text">9.5.1 测试存储和压缩</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-企业级调优"><span class="nav-number">13.</span> <span class="nav-text">10 . 企业级调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-Hive实战"><span class="nav-number">14.</span> <span class="nav-text">11. Hive实战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-1-需求描述"><span class="nav-number">14.1.</span> <span class="nav-text">11.1 需求描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-2-数据结构"><span class="nav-number">14.2.</span> <span class="nav-text">11.2 数据结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-准备工作"><span class="nav-number">14.3.</span> <span class="nav-text">11.3 准备工作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#11-3-1-ETL"><span class="nav-number">14.3.1.</span> <span class="nav-text">11.3.1 ETL</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#11-3-2-准备表"><span class="nav-number">14.3.2.</span> <span class="nav-text">11.3.2 准备表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#11-3-3-安装Tez引擎"><span class="nav-number">14.3.3.</span> <span class="nav-text">11.3.3 安装Tez引擎</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">柒7七</span>

  
</div>

<!--
  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>
-->



        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 访问总量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '7HjF3J8nerPuC6IIPSTD2MyU-gzGzoHsz',
        appKey: 'HON17u5VRHg6TQoMXv4qzkYT',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

  <!-- 动态背景-->
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
</body>
</html>
